<h2>The big O notation</h2>
<p>Suppose you need to choose one of several algorithms to solve a problem. How can you pick the best one? To do it, you need to measure the algorithm efficiency somehow.</p>

<p>One of the options might be to measure the time your program needs to process its input. However, different computers may take different time to process the same data. Furthermore, the processing time may depend on the data itself. We obviously need something more universal. So, let's try to estimate the efficiency using <strong>big O notation.</strong></p>

<h5 id="input-size">Input size</h5>

<p>What does an algorithm usually do? It makes some calculations. Let's call <strong>operations </strong>the basic actions, such as addition, multiplication, comparison, variable assignment, etc. Of course, the calculation time depends on the machine, but it doesn't matter now because we want to compare algorithms, not machines. Now let's try to estimate the number of operations in an algorithm!<img alt="" height="481" name="2.svg" src="https://ucarecdn.com/f4233b0c-4d4e-498e-84ae-772c9ed09c39/" style="float: right;" width="455"></p>

<p>Buses aren't always punctual, are they? One day it may happen that they are there on time, while the other day they will take a lifetime to arrive. You can't blame solely the driver for that: the time of the trip depends directly on the number of passengers on the bus. The more passengers, the more stops, the longer the time to arrive. Likewise, the running time of an algorithm depends on the <strong>input data.</strong> Naturally, the program will take a different time to proceed with 10 or 1 000 000 numbers. We will use the term <strong>input size</strong> as a proxy measure of the size of input data. If you need to work with <strong>m </strong>numbers, then <strong>m</strong> is the input size. The input size isn’t always the amount of the input data itself. If you need to find the first <strong><strong><strong><strong><span class="math-tex">\(n\)</span></strong></strong></strong></strong> <a target="_blank" target="_blank" target="_blank" target="_blank" target="_blank" href="https://en.wikipedia.org/wiki/Prime_number" rel="noopener noreferrer nofollow">prime numbers</a>, then searching for 10 first primes or 10 000 first primes will also take a different time, however, you only enter a single number <strong>n</strong> as input. In such cases, that number’s value is typically considered the input size.</p>

<p>If we can estimate how the number of operations depends on the input size, we will have a machine-independent measure of algorithm complexity. This is exactly what we need! Also, if we want to find a good algorithm, we are mostly interested in its behavior with big data. For this, we can compare the behavior of the algorithm’s running time with some standard functions.</p>

<h5 id="big-o-notation">Big O notation</h5>

<p>As we already mentioned, we will use the <strong>big O notation</strong> to measure the efficiency of algorithms. As a matter of fact, we have borrowed this symbol from mathematics; however, we shall not worry about the mathematical meaning or definition of the big O. Less formally, we can say that an algorithm has the <strong>time complexity</strong> <span class="math-tex">\(O(f(n))\)</span> if its number of operations grows bigger similar to (or slower than) the function <span class="math-tex">\(f(n)\)</span> when the input size <span class="math-tex">\(n\)</span> is a large number. In order to avoid unnecessary abstractness, let's consider the following task: given a <span class="math-tex">\(n\times n\)</span> table with integers in its cells. Find the number <span class="math-tex">\(k\)</span> in the given table.</p>

<p><img alt="" height="205" name="3.svg" src="https://ucarecdn.com/e619ff5f-8741-42a5-bf9d-c6abc51a613d/" style="float: right;" width="357"></p>

<p>Alice and Bob have come up with their own algorithms to solve the problem. Bob's algorithm consists in scanning every cell of the table and checking if the corresponding value is equal to <span class="math-tex">\(k\)</span>. Well, this implies a maximum of <span class="math-tex">\(n^2\)</span> comparisons, which means that the time complexity of Bob's algorithm is <span class="math-tex">\(O(n^2)\)</span>. On the other hand, Alice somehow knows earlier in which column the number <span class="math-tex">\(k\)</span> will be located, hence, she only needs to scan the elements of that column. A column consists of <span class="math-tex">\(n\)</span> cells, meaning that Alice's algorithm will take <span class="math-tex">\(O(n)\)</span> time.</p>

<p>Basically, on a table <span class="math-tex">\(2\times 2\)</span>, Bob will have to perform a maximum of <span class="math-tex">\(4\)</span> operations; meanwhile, Alice will perform no more than <span class="math-tex">\(2\)</span>. Not a big difference really, is it? What if we have a table <span class="math-tex">\(n\times n\)</span> for a large <span class="math-tex">\(n\)</span>? In this case, <span class="math-tex">\(n^2\)</span> will be considerably bigger than <span class="math-tex">\(n\)</span>, as shown below. This is exactly what determines the efficiency of an algorithm – the way it behaves with large input sizes. Hence, we conclude that Alice's algorithm is faster than Bob's, as the big O notation suggests.</p>

<p>However, a simple question arises: why can't we write simply <span class="math-tex">\(n^2\)</span> or <span class="math-tex">\(n\)</span> for the complexities? Why do we need to add this beautiful round letter in front of these functions? Well, imagine that the element <span class="math-tex">\(k\)</span> is placed in the first cell of the table. Bob will find it immediately and terminate his algorithm. How many steps does he perform: <span class="math-tex">\(n^2\)</span>? No, just one.</p>

<p style="text-align: center;"><img alt="" height="358" name="График_обновленный_.svg" src="https://ucarecdn.com/57ac2090-8c2c-4bb9-94df-948b068ab3c7/" width="745"></p>

<p>That is why we use the big O: roughly speaking, it describes the upper bound for the function's growth rate. This is one of the big O notation's essential advantages. It means that you can calculate how much time to allocate for processing a certain amount of input data and be sure that the algorithm will process it all in due time. In practice, an algorithm might sometimes work even better than what the big O notation shows for its complexity, but not worse.</p>

<h5 id="common-growth-rates">Common growth rates</h5>

<p>Below are, from best to worse, some common values of the big O function for the time complexity, also known as complexity classes.</p>

<ul>
	<li><span class="math-tex">\(O(1)\)</span> (<strong>constant time</strong>). The algorithm performs a constant number of operations. Maybe one, two, twenty-six, or two hundred – it doesn't matter. What is important is that it doesn't depend on the input size. Typical algorithms of this class include calculating the answer using a direct formula, printing a couple of values, all letters of the English alphabet, etc.</li>
	<li><span class="math-tex">\(O(\log n)\)</span> (<strong>logarithmic time</strong>). Perhaps a quick reminder on logarithms is necessary. We usually refer to logarithms of base 2; however, the base does not affect the class. By definition, <span class="math-tex">\(\log_2n\)</span> equals the number of times <span class="math-tex">\(n\)</span> must be divided by <span class="math-tex">\(2\)</span> to get <span class="math-tex">\(1\)</span>. That being said, it should not be difficult to guess that such algorithms divide the input size into <strong>halves </strong>at each step. They are relatively fast: if the size of the input is huge, say, <span class="math-tex">\(2^{31}\)</span> (programmers should know the importance of this number), the algorithm will perform approximately <span class="math-tex">\(\log_2(2^{31}) = 31\)</span> operations, which is pretty effective.</li>
	<li><span class="math-tex">\(O(n)\)</span> (<strong>linear time</strong>). The time is proportional to the input size, i.e., the time grows linearly as the input size increases. Often, such algorithms are iterated only once. They occur quite frequently, because it is usually necessary to go through every input element before calculating the final answer. This makes the <span class="math-tex">\(O(n)\)</span> class one of the most effective classes in practice.</li>
	<li><span class="math-tex">\(O(n^2)\)</span> (<strong>quadratic time</strong>). Normally, such algorithms go through all pairs of input elements. Why? Well, mathematics is generous, it constantly provides us with important results: in this case, basic maths confirms that the number of unordered pairs in a set of <span class="math-tex">\(n\)</span> elements is equal to <span class="math-tex">\(\frac{n(n-1)}{2}\)</span>, which, as we will learn later in this topic, is <span class="math-tex">\(O(n^2)\)</span>. If you find it scary or difficult to understand, it is completely normal, it happens to the best of us. On the other hand, for those who are familiar with programming terms, the following sentence might come in handy: quadratic time algorithms usually contain two nested loops. </li>
	<li><span class="math-tex">\(O(2^n)\)</span> (<strong>exponential time</strong>). Just in case, let's mention that <span class="math-tex">\(2^n\)</span> is the same as multiplying <span class="math-tex">\(2\)</span> by itself <span class="math-tex">\(n\)</span> times. Again, maths states that the number of subsets of a set of <span class="math-tex">\(n\)</span> elements is equal to <span class="math-tex">\(2^n\)</span>, therefore, it is reasonable to expect that such algorithms scan all the subsets of the input elements. It is worth noting that this class is extremely inefficient in practice; even for small input sizes, the time taken by the algorithm will be remarkably high.</li>
</ul>

<p>There are also other less common complexity classes, which you will come across in some following topics: </p>

<ul>
	<li> <span class="math-tex">\(O(\sqrt n)\)</span> (<strong>square root time</strong>);</li>
	<li> <span class="math-tex">\(O(n \log n)\)</span> (<strong>log-linear time</strong>);</li>
	<li><span class="math-tex">\(O(n^k)\)</span> (<strong>polynomial time</strong>);</li>
	<li> <span class="math-tex">\(O(n!)\)</span> (<strong>factorial time)</strong>.</li>
</ul>

<p>Now let's gather all the classes together and sort them from the best to the worst, so that you remember which ones are the most effective, and which ones you should stay away from.</p>

<p style="text-align: center;"><img alt="" height="102" name="1.svg" src="https://ucarecdn.com/a59bc4ff-df5e-460b-9e58-27e88e9ae228/" width="862"></p>

<h5 id="calculating-complexity">Calculating complexity</h5>

<p>Let's look at a simple example. You want to find the maximum of <span class="math-tex">\(n\)</span> numbers. You will probably decide to go through them and compare every new element with the maximum so far. You will make <span class="math-tex">\(n\)</span> comparisons, so the time complexity is <span class="math-tex">\(O(n)\)</span>.</p>

<p>However, algorithms are usually quite complex and consist of several steps, whose time complexity may belong to different time complexity classes from the list above. Therefore, to be able to calculate complexities by yourself, it is essential for you to get familiar with the basic properties of the Big O:</p>

<ul>
	<li><strong>Ignore the constants.</strong> As we discussed above, while calculating complexities, we focus solely on the behavior of our algorithm with large input sizes. Therefore, repeating some steps a constant number of times does not affect the complexity. For example, if you traverse <span class="math-tex">\(n\)</span> elements <span class="math-tex">\(5\)</span> times, we say that the algorithm's time complexity is <span class="math-tex">\(O(n)\)</span>, and not <span class="math-tex">\(O(5n)\)</span>. Indeed, there is no significant difference between 1 000 000 000 and 5 000 000 000 operations performed by the algorithm. In either case, we conclude that it is relatively slow. Formally, we write <span class="math-tex">\(c\cdot O(n) = O(n)\)</span>. It is similar for the rest of the complexity classes.</li>
	<li>
	<p><strong>Applying a procedure <strong><span class="math-tex">\(n\)</span></strong> times. </strong>What if you need to go over <span class="math-tex">\(n\)</span> elements <span class="math-tex">\(n\)</span> times? It is not a constant anymore, as it depends on the input size. In this case, the time complexity becomes <span class="math-tex">\(O(n^2)\)</span>. It's simple: you do <span class="math-tex">\(n\)</span> times an action proportional to <span class="math-tex">\(n\)</span>, which means the result is proportional to <span class="math-tex">\(n^2\)</span>. In big O notation, we write it as <span class="math-tex">\(n\cdot O(n) = O(n^2)\)</span>.</p>
	</li>
	<li><strong>Smaller terms do not matter. </strong>Another common case is when after doing some actions, you need to do something else. For instance, you traverse <span class="math-tex">\(n\)</span> elements <span class="math-tex">\(n\)</span> times and then traverse <span class="math-tex">\(n\)</span> elements again. In this case, the complexity is still <span class="math-tex">\(O(n^2)\)</span>. Additional <span class="math-tex">\(n\)</span> actions do not affect your complexity, which is proportional to <span class="math-tex">\(n^2\)</span>. In big O notation, it looks like this: <span class="math-tex">\(O(n)+O(n^2) = O(n^2)\)</span>. All in all, always keep the largest term in Big O and forget about all others. It is rather easy to understand which terms are larger based on the order provided in the previous section. Naturally, a question arises: why is it correct to ignore the smaller terms? Let's illustrate the example above:</li>
</ul>

<p style="text-align: center;"><img alt="" height="340" name="Big_O(correct).svg" src="https://ucarecdn.com/27c7e5ef-eb2d-48ca-8a56-e5423589515a/" width="750"></p>

<p>The images show that when the input size <span class="math-tex">\(n\)</span> is large, the graphs of <span class="math-tex">\(n^2\)</span> and <span class="math-tex">\(n^2+n\)</span> almost coincide (their growth rate is similar). As for <span class="math-tex">\(n^2\)</span>, for large <span class="math-tex">\(n\)</span> this value is considerably greater than <span class="math-tex">\(n\)</span>, therefore adding <span class="math-tex">\(n\)</span> to it does not affect the value of the function much. This is why we can rightfully write <span class="math-tex">\(O(n^2)\)</span> instead of <span class="math-tex">\(O(n^2+n)\)</span>.</p>

<h5 id="conclusion">Conclusion</h5>

<p>Big O notation is an essential instrument for algorithm performance evaluation. We can use it to assess both the time and the memory complexity. The greatest advantage of big O notation is that it classifies an algorithm rather than gives you a real running time in seconds or required memory in megabytes.</p>

<p>We should note that it is completely normal if you still find the concept of Big O a bit confusing. It is similar to reading the rules of a board game for the first time without actually having the board in front of you. As soon as you start playing, you will better realize the meaning of those rules. Analogously, in the following topics on algorithms, we will describe in detail how to calculate algorithm complexity. That will definitely lead to a better understanding of this topic as well. In a nutshell, we hope this topic hasn't demotivated you, on the contrary, you should be motivated and hungrier for more.</p>
